# Per-Attack EVAL Equal Error Rates (ASVspoof 2019 LA)

This file reports per-attack EER (%) for all four systems on the evaluation
partition of ASVspoof 2019 LA, grouped by attack ID (A07â€“A19).

The values reflect the CSV generated by `08_collect_results.py`.

---

## ðŸŽ¯ Per-Attack EER Table

| Attack | LFCC+GMM | MFCC+GMM | LFCC+CNN | MFCC+CNN |
|--------|-----------|------------|-------------|-------------|
| **A07** | 0.33 | 6.49 | 0.01 | 13.41 |
| **A08** | 0.29 | 2.41 | 0.02 | 22.34 |
| **A09** | 0.06 | 0.02 | 0.02 | 6.49 |
| **A10** | 12.70 | 9.18 | 14.60 | 14.47 |
| **A11** | 2.28 | 1.57 | 2.36 | 9.62 |
| **A12** | 1.29 | 0.69 | 17.68 | 14.59 |
| **A13** | 6.08 | 5.82 | 28.47 | 23.47 |
| **A14** | 8.21 | 3.77 | 0.85 | 26.76 |
| **A15** | 16.08 | 6.29 | 1.93 | 18.05 |
| **A16** | 0.46 | 9.50 | 0.01 | 17.31 |
| **A17** | 24.04 | 34.65 | 26.87 | 33.33 |
| **A18** | 10.94 | 15.10 | 22.54 | 26.01 |
| **A19** | 5.81 | 36.08 | 0.05 | 22.53 |

---

## ðŸ“Œ Interpretation

- **LFCC + GMM** performs strongest on attacks with high-frequency artefacts (A07â€“A09, A16).
- **MFCC + GMM** is sometimes more stable across a wider range of attacks.
- **CNN models** show large variance across attacks:
  - perform very well on some (A07, A09, A16)
  - but fail strongly on others (A12, A13, A18â€“A19)
- **A17 and A19** are difficult across *all* systems â€” matching known ASVspoof benchmarks.

These detailed results support the analysis given in the report and demonstrate clear model behaviour differences across spoofing systems.