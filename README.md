# Deepfake Speech Detection on ASVspoof 2019 LA  
Lightweight LFCC/MFCC + GMM/CNN Baselines (Fully Reproducible)

This repository implements a **lightweight, fully reproducible deepfake speech detector** for the **ASVspoof 2019 Logical Access (LA)** corpus. The goal is to detect **bona fide vs spoofed** audio using **compact cepstral features** (LFCC, MFCC) and **simple classifiers** (GMMs and a shallow CNN). The full pipeline is transparent, CPU-friendly, and runs in chronological steps from `01` → `09`.

---

# 1. Project Overview

Deepfake speech generated by TTS and voice conversion systems can threaten security and trust. This project answers the following research question:

> **Can LFCC+GMM and a shallow CNN—implemented for CPU-only inference—achieve ≈10–15% EER on the ASVspoof 2019 LA evaluation set using a DEV-set fixed threshold, and how do they compare to MFCC baselines in accuracy and robustness?**

The system achieves this through:

- **Cepstral features:** LFCC / MFCC + Δ + Δ²  
- **Models:**  
  - Diagonal-covariance **GMMs**  
  - Compact **2D CNN**  
- **Strict ASVspoof protocol compliance**  
- **Fully saved, versioned run directories** for reproducibility

---

# 2. Repository Structure

repo_root/
│
├── configs/
│     └── paths.yaml
│
├── tools/
│     ├── 01_verify_asvspoof.py
│     ├── 02_build_manifest.py
│     ├── 03_verify_manifest.py
│     ├── 04_extract_features.py
│     ├── 05_gmm.py
│     ├── 06_gmm_plot_and_table.py
│     ├── 07_cnn_baseline.py
│     ├── 08_collect_results.py
│     └── 09_visualize_results.py
│
└── results/
      ├── manifests/
      ├── models/
      │      ├── gmm_lfcc/
      │      ├── gmm_mfcc/
      │      ├── cnn_lfcc/
      │      └── cnn_mfcc/
      └── summary/

---

# 3. Installation & Requirements

Python 3.9+ recommended.

Install dependencies:
pip install numpy scipy pandas scikit-learn torch torchaudio tqdm matplotlib soundfile pyyaml joblib

---

# 4. Dataset Setup

You can download the ASVspoof 2019 Logical Access (LA) dataset from the official challenge site  
or from this Kaggle mirror:

**Kaggle (LA subset):**  
https://www.kaggle.com/datasets/anishsarkar22/asvpoof-2019-dataset-la

After downloading, extract the dataset and set the path in:
configs/paths.yaml

Example:
asvspoof_root: /data/ASVspoof2019_LA  
la_protocol_dir: /data/ASVspoof2019_LA/LA/ASVspoof2019_LA_cm_protocols

Expected directory structure:
ASVspoof2019_LA_train/
ASVspoof2019_LA_dev/
ASVspoof2019_LA_eval/
LA/ASVspoof2019_LA_cm_protocols/

---

# 5. Running the Pipeline (01 → 09)

All scripts live in tools/ and are meant to run in numerical order.

---

## Step 1 — Verify dataset paths

python tools/01_verify_asvspoof.py --config configs/paths.yaml

---

## Step 2 — Build manifests

python tools/02_build_manifest.py --config configs/paths.yaml  
python tools/03_verify_manifest.py

Outputs:
results/manifests/train.csv  
results/manifests/dev.csv  
results/manifests/eval.csv  

---

## Step 3 — Extract LFCC & MFCC features

python tools/04_extract_features.py

Outputs example:
results/features/lfcc/train/*.npz  
results/features/mfcc/dev/*.npz  

Each .npz includes:
feat: [T, 60]   (static + Δ + Δ²)

---

## Step 4 — Train GMM baselines

LFCC:
python tools/05_gmm.py --feat lfcc --gmm_out_dir results/models/gmm_lfcc

MFCC:
python tools/05_gmm.py --feat mfcc --gmm_out_dir results/models/gmm_mfcc

Outputs:
run_config.json  
metrics.json  
scores_dev.csv  
scores_eval.csv  
roc_dev.csv  
roc_eval.csv  

---

## Step 5 — Train CNN baselines

LFCC:
python tools/07_cnn_baseline.py --feat lfcc --out_dir results/models/cnn_lfcc

MFCC:
python tools/07_cnn_baseline.py --feat mfcc --out_dir results/models/cnn_mfcc

Outputs:
model.pt  
metrics.json  
roc samples  
per-attack tables  

---

## Step 6 — Collect summary results

Edit paths inside tools/08_collect_results.py, then run:

python tools/08_collect_results.py

Outputs:
results/summary/headline.csv  
results/summary/per_attack_eer_eval_merged.csv  
results/summary/*.tex  

---

## Step 7 — Visualise ROC curves

python tools/09_visualize_results.py

Outputs:
results/summary/roc_dev_overlay.png  
results/summary/roc_eval_overlay.png  

---

# 6. Example Input & Output (End-to-End Demonstration)

The folder:
results/models/<model>/<run_tag>/

contains two files showing how the algorithm performs on every utterance:

- scores_dev.csv
- scores_eval.csv

Each of these files contains three columns:

utt_id, score, label_int

Where:
- utt_id = the name of the audio file processed (this is the example input)
- score = the model output (log-likelihood-ratio for GMM, or logit for CNN)
- label_int = the ground truth label provided by the ASVspoof protocol  
               0 = bona fide (real speech)  
               1 = spoof (fake or generated speech)

Therefore:
- Ground truth ALWAYS comes from label_int.
- The prediction is obtained by applying the DEV-set threshold to the score.
- We compare prediction vs label_int to determine correctness.

Below are two demonstration examples taken directly from how the algorithm works. It will shows how the SAME evaluation utterance is processed by ALL FOUR systems (GMM_LFCC, GMM_MFCC, CNN_LFCC, CNN_MFCC).

---

## Example 1 — Spoof Input (from scores_eval.csv)

Example row inside scores_eval.csv:
utt_id:      LA_E_2834763 
label_int:   1

Interpretation:
- label_int = 1 → ground truth = spoof

Decision rule:
If score > threshold → predicted = spoof  
If score ≤ threshold → predicted = bona fide

Ground truth always comes from label_int in scores_eval.csv.

---

### Model 1 — GMM_MFCC

From GMM_MFCC scores_eval.csv:

- utt_id: LA_E_2834763  
- score: 0.569593608379364  
- label_int: 1  (ground truth = spoof)

From GMM_MFCC metrics.json:

- thr = -1.2820403575897217

Apply decision rule:

- 0.569593608379364 > -1.2820403575897217  
  → predicted = spoof (1)

Compare with ground truth:

- predicted = spoof (1)  
- ground truth = spoof (1)  

Result: **Correct**

---

### Model 2 — GMM_LFCC

From GMM_LFCC scores_eval.csv:

- utt_id: LA_E_2834763  
- score: -2.8321845531463623  
- label_int: 1  (ground truth = spoof)

From GMM_LFCC metrics.json:

- thr = -1.9212855100631714

Apply decision rule:

- -2.8321845531463623 ≤ -1.9212855100631714  
  → predicted = bona fide (0)

Compare with ground truth:

- predicted = bona fide (0)  
- ground truth = spoof (1)  

Result: **Incorrect (missed spoof)**

---

### Model 3 — CNN_MFCC

From CNN_MFCC scores_eval.csv:

- utt_id: LA_E_2834763  
- score: 0.8827216029167175  
- label_int: 1  (ground truth = spoof)

From CNN_MFCC metrics.json:

- thr = -2.443451166152954

Apply decision rule:

- 0.8827216029167175 > -2.443451166152954  
  → predicted = spoof (1)

Compare with ground truth:

- predicted = spoof (1)  
- ground truth = spoof (1)  

Result: **Correct**

---

### Model 4 — CNN_LFCC

From CNN_LFCC scores_eval.csv:

- utt_id: LA_E_2834763  
- score: 0.09152206778526306  
- label_int: 1  (ground truth = spoof)

From CNN_LFCC metrics.json:

- thr = 1.0877103805541992

Apply decision rule:

- 0.09152206778526306 ≤ 1.0877103805541992  
  → predicted = bona fide (0)

Compare with ground truth:

- predicted = bona fide (0)  
- ground truth = spoof (1)  

Result: **Incorrect (missed spoof)**

---

### Summary for LA_E_2834763

On the same spoof utterance (LA_E_2834763):

- **GMM_MFCC**: predicts spoof → Correct  
- **GMM_LFCC**: predicts bona fide → Incorrect  
- **CNN_MFCC**: predicts spoof → Correct  
- **CNN_LFCC**: predicts bona fide → Incorrect  

This concrete example shows how the same input audio can lead to different decisions depending on:
- the feature representation (LFCC vs MFCC)
- the model type (GMM vs CNN)
and how scores and thresholds in the results folders translate into actual spoof / bona fide decisions.


## Summary of How the Algorithm Produces These Outputs

This demonstrates exactly how the system processes a real ASVspoof evaluation file:

1. The audio file listed in the ASVspoof protocol (utt_id) is taken as input.
2. Features (LFCC or MFCC with Δ and Δ²) are extracted and saved in results/features/.
3. The chosen model (GMM or CNN) produces a numerical score.
4. The DEV-set equal-error-rate threshold from metrics.json is applied:
   - score > threshold → predicted spoof
   - score ≤ threshold → predicted bona fide
5. Ground truth is read directly from label_int in the scores CSV.
6. The prediction is compared with label_int to determine correctness.

This example shows a full end-to-end input → processing → final prediction workflow.




# 7. Testing & Exploration

Included analyses:

- LFCC vs MFCC  
- GMM vs CNN  
- Attack-wise EER (A07–A19)  
- DEV → EVAL generalisation  
- ROC overlays  
- Low-FPR behaviour  

Key findings:

- LFCC+GMM strongest at low FPR  
- CNN overfits DEV  
- Attacks A17 & A19 are hardest  
- LFCC better captures high-frequency artefacts  

---

# 8. Planned Future Extensions

- RawNet2  
- RawNetLite  
- Whisper-AAiST anti-spoofing model  

---

# 9. Reproducibility

Every run creates a full directory under results/models/<model>/<tag>/ including:
run_config.json  
metrics.json  
scores_dev/eval.csv  
roc samples  
per-attack EER  
model weights (CNN)  
feature scalers (GMM)  

Full pipeline:

python tools/01_verify_asvspoof.py  
python tools/02_build_manifest.py  
python tools/03_verify_manifest.py  
python tools/04_extract_features.py  
python tools/05_gmm.py ...  
python tools/07_cnn_baseline.py ...  
python tools/08_collect_results.py  
python tools/09_visualize_results.py  

---

# 10. Contributions

This project contributes:

- A complete scripted pipeline (01–09)  
- LFCC/MFCC extraction (static + Δ + Δ²)  
- GMM with balanced sampling and capped frame pooling  
- Lightweight 2D CNN baseline  
- Fully reproducible run directories  
- Tools for plotting, per-attack tables, and summary results  
- A clean CPU-friendly ASVspoof baseline for research/teaching  

---